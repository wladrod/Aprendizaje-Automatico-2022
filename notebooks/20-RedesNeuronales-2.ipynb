{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 4: Redes Neuronales</h1>\n",
    "    <h1>Redes Neuronales 2</h1>\n",
    "    <br>\n",
    "    <h5>Prof. Wladimir Rodr铆guez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computaci贸n</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de las Redes Neuronales: Propagaci贸n Hacia Atr谩s \"Backpropagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci贸n se explicaran los conceptos de la funci贸n de costo log铆stico y el algoritmo de propagaci贸n hacia atr谩s \"Backpropagation\" que se implemantaron en la clase anterior para aprender los pesos de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular la funci贸n de costo log铆stico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que se implement贸 el Perceptron Multicapa para realizar clasificaci贸n m煤ltiple, la salida es un vector de $t$ elementos usando la codificaci贸n \"one-hot\", por ejemplo la salida de la capa de salida para un ejemplo de la clase 2:\n",
    "\n",
    "$$a^{(salida)}=\\left[\\\n",
    "\\begin{array}{ll}\n",
    "      0.1 \\\\\n",
    "      0.9 \\\\\n",
    "      \\vdots \\\\\n",
    "      0.3 \\\\\n",
    "\\end{array} \n",
    "\\right], \\ y = \\left[\\\n",
    "\\begin{array}{ll}\n",
    "      0 \\\\\n",
    "      1 \\\\\n",
    "      \\vdots \\\\\n",
    "      0 \\\\\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Por lo que es necesario generalizar la funci贸n de costo log铆stico a todas las $t$ unidades de activaci贸n de la red. Por lo que la funci贸n de costo es:\n",
    "\n",
    "$$L(\\boldsymbol{W})=\\frac{1}{n}\\sum_{1}^{n}\\sum_{j=1}^{t}(y^{[i]}_j - a_j^{(salida)[i]})^2$$\n",
    "\n",
    "Recordar que nuestro objetivo es minimizar la funci贸n de costo $L(\\boldsymbol{W})$; por lo tanto, se necesita calcular la derivada parcial de los par谩metros $\\boldsymbol{W}$ con respecto a cada peso para cada capa en la red:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{j,l}^{(l)}}L(\\boldsymbol{W})$$\n",
    "\n",
    "A continuaci贸n, se presenta el algoritmo de propagaci贸n hacia atr谩s \"backpropagation\", que permite calcular esas derivadas parciales para minimizar la funci贸n de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de Propagaci贸n Hacia Atr谩s \"Backpropagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esencia, podemos pensar en la propagaci贸n hacia atr谩s \"backpropagation\" como un enfoque computacionalmente muy eficiente para calcular las derivadas parciales de una funci贸n de costo compleja en redes neuronales multicapa. Aqu铆, nuestro objetivo es usar esas derivadas\n",
    "para aprender los coeficientes de peso para parametrizar una red neuronal multicapa.\n",
    "\n",
    "El desaf铆o en la parametrizaci贸n de redes neuronales es que se esta t铆picamente tratando con una gran cantidad de coeficientes de peso en un espacio de atributos de muchas dimensiones. En contraste con las funciones de costo de neuronales de una sola capa\n",
    "redes como Adaline o regresi贸n log铆stica, la superficie de error de una funci贸n de costo de una red neuronal no es convexa o lisa con respecto a los par谩metros. Hay muchos baches en esta superficie de costo multi-dimensional (m铆nimos locales) que tenemos que superar para encontrar el m铆nimo de la funci贸n de costo.\n",
    "\n",
    "La regla de la cadena es un enfoque para calcular la derivada de una funci贸n anidada compleja, como $f(g(x))$, de la siguiente manera:\n",
    "\n",
    "$$\\frac{d}{dx}[f(g(x))]=\\frac{df}{dx}\\cdot\\frac{dg}{dx}$$\n",
    "\n",
    "De manera similar, se puede usar la regla de cadena para una composici贸n de funci贸n arbitrariamente larga. Por ejemplo, supongamos que tenemos cinco funciones diferentes, $f(x)$, $g(x)$, $h(x)$, $u(x)$ y $v(x)$, y que $F$ sea la composici贸n de la funci贸n: $F(x) = f(g(h(u(v(x)))))$. Aplicando la regla de cadena, podemos calcular la derivada de esta funci贸n de la siguiente manera:\n",
    "\n",
    "$$\\frac{dF}{dx}=\\frac{d}{dx}F(x)=\\frac{d}{dx}f(g(h(u(v(x)))))=\\frac{df}{dg}\\cdot\\frac{dg}{dh}\\cdot\\frac{dh}{du}\\cdot\\frac{du}{dv}\\cdot\\frac{dv}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando  Redes Neuronales usando Propagaci贸n Hacia Atr谩s \"Backpropagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci贸n, explicaremos la matem谩tica de propagaci贸n hacia atr谩s \"backpropagation\" para entender c贸mo se pueden aprender los pesos en una red neuronal de manera muy eficiente.\n",
    "\n",
    "Anteriormente vimos como calcular el costo como la diferencia entre la activaci贸n de la capa de salida y y la etiqueta de clase objetivo. Ahora veremos como trabaja el algoritmo de propagaci贸n hacia atr谩s para actualizar los pesos en nuestro modelo de Perceptr贸n Multicapa. \n",
    "\n",
    "Primero necesitamos aplicar la propagaci贸n hacia adelante para obtener la activaci贸n de la capa de salida, que formulamos de la siguiente manera:\n",
    "\n",
    "$$\\boldsymbol{Z}^{(oculta)}=\\boldsymbol{A}^{(entrada)}\\boldsymbol{W}^{(oculta)T}\\ \\ (entrada\\ a\\ la\\ capa\\ oculta)$$\n",
    "\n",
    "$$\\boldsymbol{A}^{(oculta)}=\\phi(\\boldsymbol{Z}^{(oculta)})\\ \\ (activaci贸n\\ de\\ la\\ capa\\ oculta)$$\n",
    "\n",
    "$$\\boldsymbol{Z}^{(salida)}=\\boldsymbol{A}^{(oculta)}\\boldsymbol{W}^{(salida)T}\\ \\ (entrada\\ a\\ la\\ capa\\ de \\ salida)$$\n",
    "\n",
    "$$\\boldsymbol{A}^{(salida)}=\\phi(\\boldsymbol{Z}^{(salida)})\\ \\ (activaci贸n\\ de\\ la\\ capa\\ \\ de\\ salida)$$\n",
    "\n",
    "La siguiente figura muestra este proceso:\n",
    "\n",
    "<img src=\"../figuras/MLP_backpropagation.png\" width=\"75%\">\n",
    "\n",
    "En propagaci贸n hacia atr谩s, propagamos el error de derecha a izquierda. Podemos pensar en esto como una aplicaci贸n de la regla de la cadena al c谩lculo de la propagaci贸n hacia adelante para calcular el gradiente de la p茅rdida con respecto a los pesos del modelo (y las unidades de sesgo). Para simplificar, ilustraremos este proceso para la derivada parcial utilizada para actualizar el primer peso en la matriz de peso de la capa de salida. Las rutas del c谩lculo que propagamos hacia atr谩s se resaltan mediante las flechas en negrita a continuaci贸n:\n",
    "\n",
    "<img src=\"../figuras/MLP_backpropagation_2.png\" width=\"75%\">\n",
    "\n",
    "El gradiente para el peso $ w_{1,1}^{(s)}$ de la capa de salida:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{1,1}^{(s)}} = \\frac{\\partial L}{\\partial a_1^{(s)}}\\cdot \\frac{\\partial a_1^{(s)}}{\\partial w_{1,1}^{(s)}}$$\n",
    "\n",
    "\n",
    "Si incluimos la entrada $z$ a la neurona:\n",
    "\n",
    "<img src=\"../figuras/neurona_expandida.png\" width=\"25%\">\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{1,1}^{(s)}} = \\frac{\\partial L}{\\partial a_1^{(s)}}\\cdot \\frac{\\partial a_1^{(s)}}{\\partial z_1^{(s)}}\\cdot \\frac{\\partial z_1^{(s)}}{\\partial w_{1,1}^{(s)}}$$\n",
    "\n",
    "\n",
    "Para calcular esta derivada parcial, comenzaremos con $\\frac{\\partial L}{\\partial a_1^{(s)}}$, que es la derivada parcial de la funci贸n de perdida MSE.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_1^{(s)}} = \\frac{\\partial L}{\\partial a_1^{(s)}}(y_1 - a_1^{(s)})^2 = 2( a_1^{(s)} - y)$$\n",
    "\n",
    "El pr贸ximo t茅rmino es la derivada de la funci贸n de activaci贸n log铆stica que se usa en la capa de salida:\n",
    "\n",
    "$$\\frac{\\partial a_1^{(s)}}{\\partial z_1^{(s)}} = \\frac{\\partial}{\\partial z_1^{(s)}} \\frac{1}{1 + e^{z_1^{(s)}}} = \\cdots = \\left(\\frac{1}{1 + e^{z_1^{(s)}}}\\right) \\left(1 - \\frac{1}{1 + e^{z_1^{(s)}}}\\right)$$\n",
    "\n",
    "$$= a_1^{(s)}(1 - a_1^{(s)})$$\n",
    "\n",
    "Por 煤ltimo, calculamos la derivada de la entrada con respecto al peso:\n",
    "\n",
    "$$\\frac{\\partial z_1^{(s)}}{\\partial w_{1,1}^{(s)}} = \\frac{\\partial}{\\partial w_{1,1}^{(s)}}a_1^{(o)}w_{1,1}^{(s)} + b_1^{(s)} = a_1^{(o)}$$\n",
    "\n",
    "Poniendo todo junto, obtenemos lo siguiente:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{1,1}^{(s)}} = \\frac{\\partial L}{\\partial a_1^{(s)}}\\cdot \\frac{\\partial a_1^{(s)}}{\\partial z_1^{(s)}}\\cdot \\frac{\\partial z_1^{(s)}}{\\partial w_{1,1}^{(s)}} = 2( a_1^{(s)} - y) \\cdot a_1^{(s)}(1 - a_1^{(s)}) \\cdot a_1^{(o)}$$\n",
    "\n",
    "Luego usamos este valor para actualizar el peso a trav茅s de la conocida actualizaci贸n de descenso de gradiente estoc谩stico con una tasa de aprendizaje de :\n",
    "\n",
    "$$w_{1,1}^{(s)} = w_{1,1}^{(s)} - \\eta \\frac{\\partial L}{\\partial w_{1,1}^{(s)}}$$\n",
    "\n",
    "\n",
    "A continuacion se ilustra c贸mo calcular la derivada parcial de la p茅rdida con respecto al primer peso de la capa oculta:\n",
    "\n",
    "<img src=\"../figuras/MLP_backpropagation_3.png\" width=\"75%\">\n",
    "\n",
    "Es importante resaltar que dado que el peso $w_{1,1}^{(o)}$ est谩 conectado a ambos nodos de salida, tenemos que usar la regla de la cadena multivariable para sumar las dos rutas resaltadas con flechas en negrita.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{1,1}^{(o)}} = \\frac{\\partial L}{\\partial a_1^{(s)}}\\cdot \\frac{\\partial a_1^{(s)}}{\\partial z_1^{(s)}}\\cdot \\frac{\\partial z_1^{(s)}}{\\partial a_1^{(o)}} \\cdot \\frac{\\partial a_1^{(o)}}{\\partial z_1^{(o)}} \\cdot \\frac{\\partial z_1^{(o)}}{\\partial w_{1,1}^{(o)}}\\\\\n",
    "\\qquad \\; + \\frac{\\partial L}{\\partial a_2^{(s)}}\\cdot \\frac{\\partial a_2^{(s)}}{\\partial z_2^{(s)}}\\cdot \\frac{\\partial z_2^{(s)}}{\\partial a_1^{(o)}} \\cdot \\frac{\\partial a_1^{(o)}}{\\partial z_1^{(o)}} \\cdot \\frac{\\partial z_1^{(o)}}{\\partial w_{1,1}^{(o)}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
