{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf06c56-f59d-4fd5-9b56-41e0cef8c24a",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 5: Aprendizaje por Refuerzo</h1>\n",
    "    <h1>Q Learning</h1>\n",
    "    <h1></h1>\n",
    "    <h5>Prof. Wladimir Rodriguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computaci√≥n</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69aee51-3380-4b99-b5a8-b246ddfda090",
   "metadata": {},
   "source": [
    "`Q-learning` es un algoritmo de aprendizaje por refuerzo sin modelo para aprender el valor de una acci√≥n en un estado particular. No requiere un modelo del ambiente (por lo tanto, \"sin modelo\"), y puede manejar problemas con transiciones estoc√°sticas y recompensas sin requerir adaptaciones.\n",
    "\n",
    "Para cualquier proceso de decisi√≥n de Markov finito (PDMF), `Q-learning` encuentra una pol√≠tica √≥ptima en el sentido de maximizar el valor esperado de la recompensa total en todos y cada uno de los pasos sucesivos, comenzando desde el estado actual. `Q-learning` puede identificar una pol√≠tica de selecci√≥n de acciones √≥ptima para cualquier PDMF dado, con un tiempo de exploraci√≥n infinito y una pol√≠tica parcialmente aleatoria.`Q` se refiere a la funci√≥n que calcula el algoritmo: las recompensas esperadas por una acci√≥n realizada en un estado determinado.\n",
    "\n",
    "- El `Q-Learning` es el algoritmo Aprendizaje por Refuerzo que:\n",
    "   - Entrena una `funci√≥n Q`, que contiene, como memoria interna, una `tabla Q` la cual contiene todos los valores del par estado-acci√≥n.\n",
    "    \n",
    "   - Dado un estado y una acci√≥n, nuestra `funci√≥n Q` buscar√° en su tabla Q el valor correspondiente.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/tablaQ.png'/>\n",
    "</center>\n",
    "\n",
    "  - Cuando finaliza el entrenamiento, tenemos una `funci√≥n Q` √≥ptima, por lo tanto, una `tabla Q` √≥ptima.\n",
    "    \n",
    "- Y si tenemos una `funci√≥n Q` √≥ptima, tenemos una pol√≠tica √≥ptima, ya que sabemos para cada estado, cu√°l es la mejor acci√≥n a tomar.\n",
    "\n",
    "$$\\pi^*(s) = \\underset{a}{argmaxQ^*}(s,a)$$\n",
    "\n",
    "Pero, al principio,¬†nuestra `tabla Q` es in√∫til, ya que da un valor arbitrario para cada par estado-acci√≥n¬†(la mayor√≠a de las veces inicializamos la `tabla q` con 0). Pero, a medida que exploremos el ambiente y actualicemos nuestra `tabla Q`, nos dar√° mejores y mejores aproximaciones.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/frozenlakeQ.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591775a-76b3-4fbe-9fbb-ff35207ce1fb",
   "metadata": {},
   "source": [
    "## Ejemplo de `Q learning` utilizando el ambiente `Frozen Lake` de la librer√≠a `Gym'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30c320-9c41-4bf2-8a11-1ffd0bee250a",
   "metadata": {},
   "source": [
    "`Frozen Lake` es un ambiente simple compuesto por mosaicos, donde el agente tiene que pasar de un mosaico inicial a uno objetivo. Los mosaicos pueden ser un lago congelado seguro o un agujero que te atrapa para siempre. El agente, tiene 4 acciones posibles: ir  a la IZQUIERDA, a ABAJO, a la DERECHA o ARRIBA. El agente debe aprender a sortear los agujeros para llegar a la meta en un n√∫mero m√≠nimo de acciones. De forma predeterminada, el ambiente siempre tiene la misma configuraci√≥n. En el c√≥digo del ambiente, cada mosaico est√° representado por una letra de la siguiente manera\n",
    "\n",
    "```\n",
    "S F F F       (S: punto de entrada, seguro)\n",
    "F H F H       (F: superficie congelada, seguro)\n",
    "F F F H       (H: hueco, atrapado para siempre forever)\n",
    "H F F G       (G: meta, seguro)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7d65d-f458-49c8-b076-5f5920f5cb56",
   "metadata": {},
   "source": [
    "### Importar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de629c8-70a1-45c1-b89d-3d897004600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a940141-4fe5-4d1f-91ca-6ce9f88e2639",
   "metadata": {},
   "source": [
    "### Crear ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e82e04a-4fa3-422e-a2d5-7fb4d0ec7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "ambiente = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "ambiente.reset()\n",
    "ambiente.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b1fe9-bbf9-463f-a7f6-92b4e2d51a61",
   "metadata": {},
   "source": [
    "En `Frozen Lake` hay 16 mosaicos, lo que significa que nuestro agente se puede encontrar en 16 posiciones diferentes, llamadas estados. Para cada estado, hay 4 acciones posibles: ir ‚óÄÔ∏èIZQUIERDA, üîΩABAJO, ‚ñ∂Ô∏èDERECHA y üîºARRIBA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da272798-3d15-448b-9530-6a77c61b760f",
   "metadata": {},
   "source": [
    "#### Espacio de Estados/Observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed42205-5511-4c43-bb0d-504c204e71bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ESPACIO DE OBSERVACIONES_____ \n",
      "\n",
      "Forma del Espacio de Observaciones Discrete(16)\n",
      "Ejemplo de Observaci√≥n 2\n"
     ]
    }
   ],
   "source": [
    "ambiente.reset()\n",
    "print(\"_____ESPACIO DE OBSERVACIONES_____ \\n\")\n",
    "print(\"Forma del Espacio de Observaciones\", ambiente.observation_space)\n",
    "print(\"Ejemplo de Observaci√≥n\", ambiente.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a05714-0dc8-402e-ada9-02cfbe02edc3",
   "metadata": {},
   "source": [
    "Vemos con `Forma del Espacio de Observaciones(16)` que la observaci√≥n es un valor que representa la posici√≥n actual del **agente como fila_actual * numero_fila + columna_actual (donde tanto la fila como la columna comienzan en 0)**.\n",
    "\n",
    "Por ejemplo, la posici√≥n del objetivo en el mapa 4x4 se puede calcular de la siguiente manera: 3 * 4 + 3 = 15. El n√∫mero de observaciones posibles depende del tama√±o del mapa. **Por ejemplo, el mapa 4x4 tiene 16 posibles observaciones.**\n",
    "\n",
    "\n",
    "Por ejemplo, as√≠ es como se ve estado = 0:\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/frozenlake.png'/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667842bc-acca-460c-b006-9d24f157e230",
   "metadata": {},
   "source": [
    "#### Espacio de Acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e0c923-9097-4703-872f-66ead8a42c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ESPACIO DE ACCIONES_____ \n",
      "\n",
      "Forma del Espacio de Acciones 4\n",
      "Ejemplo de Acci√≥n 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ESPACIO DE ACCIONES_____ \\n\")\n",
    "print(\"Forma del Espacio de Acciones\", ambiente.action_space.n)\n",
    "print(\"Ejemplo de Acci√≥n\", ambiente.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540d450-2355-4e8b-8d35-7cebe6f4ce0a",
   "metadata": {},
   "source": [
    "El espacio de acci√≥n (el conjunto de acciones posibles que puede realizar el agente) es discreto con 4 acciones disponibles:\n",
    "- 0: IR A LA IZQUIERDA\n",
    "- 1: ABAJO\n",
    "- 2: IR A LA DERECHA\n",
    "- 3: ARRIBA\n",
    "\n",
    "Funci√≥n de recompensa:\n",
    "- Alcanzar la meta: +1\n",
    "- Alcanzar hoyo: 0\n",
    "- Alcanzar congelado: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f9a8fb-faca-43ff-8bc9-6e326b0bb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen  16  posibles estados\n",
      "Existen  4  posibles acciones\n"
     ]
    }
   ],
   "source": [
    "espacio_de_estados = ambiente.observation_space.n\n",
    "print(\"Existen \", espacio_de_estados, \" posibles estados\")\n",
    "\n",
    "espacio_de_accion = ambiente.action_space.n\n",
    "print(\"Existen \", espacio_de_accion, \" posibles acciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746033a-6ea7-4b38-a38e-03e22db1ae03",
   "metadata": {},
   "source": [
    "#### Crear la `tabla Q` y llenarla con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69863146-c83b-4284-a375-b8d1a1eaf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_tabla_q(espacio_de_estados, espacio_de_accion):\n",
    "  tablaQ = np.zeros((espacio_de_estados, espacio_de_accion))\n",
    "  return tablaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fea2e13-cde5-4356-86cf-053a258fcb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozenlake_tablaQ = inicializar_tabla_q(espacio_de_estados, espacio_de_accion)\n",
    "frozenlake_tablaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c13f8-6b3e-471f-a0c7-e9e5d9dce5f4",
   "metadata": {},
   "source": [
    "### Definir la Pol√≠tica `Epsilon-Avara`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a0156-75c3-4860-b08d-03a21fa0632c",
   "metadata": {},
   "source": [
    "`Epsilon-Avara` es la pol√≠tica de entrenamiento que maneja el compromiso entre exploraci√≥n/explotaci√≥n.\n",
    "\n",
    "La idea con `Epsilon Avara`:\n",
    "\n",
    "Con probabilidad $1‚Ää-‚Ää\\epsilon$: hacemos explotaci√≥n (es decir, nuestro agente selecciona la acci√≥n con el valor de par estado-acci√≥n m√°s alto).\n",
    "\n",
    "Con probabilidad $\\epsilon$: hacemos exploraci√≥n (intentando acciones aleatorias).\n",
    "\n",
    "Y a medida que avanza el entrenamiento vamos reduciendo progresivamente el valor de √©psilon ya que cada vez necesitaremos menos exploraci√≥n y m√°s explotaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63fc0eb-d6ca-47e7-81d6-d52e023731d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def politica_epsilon_avara(tablaQ, estado, epsilon):\n",
    "  # Generar un n√∫mero aleatorio entre 0 y 1\n",
    "  numero_aleatorio = random.uniform(0,1)\n",
    "  # si numero_aleatorio > mayor que epsilon --> explotaci√≥n\n",
    "  if numero_aleatorio > epsilon:\n",
    "    # Tomar la acci√≥n con el valor mayor dado el estado\n",
    "    accion = np.argmax(tablaQ[estado])\n",
    "  # else --> exploraci√≥n\n",
    "  else:\n",
    "    accion = ambiente.action_space.sample()\n",
    "  \n",
    "  return accion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd0f3b-b3eb-4a1e-a229-527156e3c5b2",
   "metadata": {},
   "source": [
    "### Definir los hiperpar√°metros\n",
    "\n",
    "Los hiperpar√°metros relacionados con la exploraci√≥n son algunos de los m√°s importantes.\n",
    "\n",
    "- Necesitamos asegurarnos de que nuestro agente **explore lo suficiente el espacio de estado** para aprender una buena aproximaci√≥n de valor, para hacer eso necesitamos tener un decaimiento progresivo del √©psilon.\n",
    "- Si disminuye el √©psilon demasiado r√°pido (tasa de decaimiento demasiado alta), **corre el riesgo de que su agente se quede atascado**, ya que su agente no explor√≥ lo suficiente el espacio de estado y, por lo tanto, no puede resolver el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165c8261-f1cd-442b-9597-026ac91243bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros de Entrenamientos\n",
    "n_episodios_entrenamiento = 10000  # Total de episodios de entrenamiento\n",
    "tasa_de_aprendizaje = 0.7          # Tasa de aprendizaje\n",
    "\n",
    "# Par√°metros de Evaluaci√≥n\n",
    "n_episodios_evaluacion = 100       # Total de episodios de prueba\n",
    "\n",
    "# Par√°metros del Ambiente\n",
    "nombre_ambiente = \"FrozenLake-v1\" # Nombre del ambiente\n",
    "max_pasos = 99                    # M√°ximo n√∫mero de pasos por episodio\n",
    "gamma = 0.95                      # Taza de Descuento\n",
    "semilla_evaluacion = []           # Semilla de evaluacion del ambiente\n",
    "\n",
    "# Par√°metros Exploraci√≥n\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability \n",
    "tasa_decaimiento = 0.0005   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaa95f-f3ad-42b7-8f46-9b47516d494c",
   "metadata": {},
   "source": [
    "#### Crear la funcion de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1416d53a-8635-4af2-bd23-c3299c3a8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(n_episodios_entrenamiento, min_epsilon, max_epsilon, tasa_decaimiento, ambiente, max_pasos, tablaQ):\n",
    "  for episodio in tqdm(range(n_episodios_entrenamiento)):\n",
    "    # Reducir epsilon (porque necesitamos menos y menos exploraci√≥n)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-tasa_decaimiento*episodio)\n",
    "    # Reiniciar el ambiente\n",
    "    estado = ambiente.reset()\n",
    "    paso = 0\n",
    "    listo = False\n",
    "\n",
    "    for paso in range(max_pasos):\n",
    "      # Seleccionar la acci√≥n usando la epsilon de politica avara\n",
    "      accion = politica_epsilon_avara(tablaQ, estado, epsilon)\n",
    "\n",
    "      # Tomar la acci√≥n y observar el nuevo estado  y la recompensa\n",
    "      nuevo_estado, recompensa, listo, info = ambiente.step(accion)\n",
    "\n",
    "      # Actualizar Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      tablaQ[estado][accion] = tablaQ[estado][accion] + tasa_de_aprendizaje * (recompensa + gamma * np.max(tablaQ[nuevo_estado]) - tablaQ[estado][accion])   \n",
    "\n",
    "      # Si listo, terminar el episodio\n",
    "      if listo:\n",
    "        break\n",
    "      \n",
    "      # Nuestro estadp es el nuevo estado\n",
    "      estado = nuevo_estado\n",
    "  return tablaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6da358-e029-4471-98ef-275a88d7d0ea",
   "metadata": {},
   "source": [
    "#### Entrenar el agente `Q Learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7620acf-a2ac-43ff-9f06-2c4e792d9122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26eb6ff61f4a48beaaa052a8e4fd97d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frozenlake_tablaQ = entrenamiento(n_episodios_entrenamiento, min_epsilon, max_epsilon, tasa_decaimiento, ambiente, max_pasos, frozenlake_tablaQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9308be2-49de-4595-a314-179bd185b19e",
   "metadata": {},
   "source": [
    "#### Observar la `tabla Q` resultante del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb7767f-bec9-4a14-b8ba-a136bbfafbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozenlake_tablaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a02870-3f92-4d39-82af-a3a65281ba03",
   "metadata": {},
   "source": [
    "#### Definir la funci√≥n de evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "925be64f-a0fb-4a90-9a86-7d0202a9b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_agente(ambiente, max_pasos, n_episodios_evaluacion, Q, semilla):\n",
    "\n",
    "  recompensa_episodio = []\n",
    "  for episode in tqdm(range(n_episodios_evaluacion)):\n",
    "    if semilla:\n",
    "      estado = ambiente.reset(seed=semilla[episodio])\n",
    "    else:\n",
    "      estado = ambiente.reset()\n",
    "    paso = 0\n",
    "    listo = False\n",
    "    recompensa_total_episodio = 0\n",
    "    \n",
    "    for paso in range(max_pasos):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      accion = np.argmax(Q[estado][:])\n",
    "      nuevo_estado, recompensa, listo, info = ambiente.step(accion)\n",
    "      recompensa_total_episodio += recompensa\n",
    "      if n_episodios_evaluacion == 1:\n",
    "          print(nuevo_estado)  \n",
    "      if listo:\n",
    "        break\n",
    "      estado = nuevo_estado\n",
    "    recompensa_episodio.append(recompensa_total_episodio)\n",
    "  recompensa_media = np.mean(recompensa_episodio)\n",
    "  recompensa_desviacion_estandar = np.std(recompensa_episodio)\n",
    "\n",
    "  return recompensa_media, recompensa_desviacion_estandar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27380862-147f-4dc3-af50-86bd7b03cfea",
   "metadata": {},
   "source": [
    "### Evaluar a nuestro agente `Q-Learning` \n",
    "\n",
    "- Normalmente deber√≠as tener una recompensa media de 1.0\n",
    "- Es relativamente f√°cil ya que el espacio de estado es realmente peque√±o (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74cf9c90-c3fc-4238-9206-d925e0ca8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092a8c8b3be0406b99af81409f8b1df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa media=1.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "recompensa_media, recompensa_desviacion_estandar = evaluar_agente(ambiente, max_pasos, n_episodios_evaluacion, frozenlake_tablaQ, semilla_evaluacion)\n",
    "print(f\"Recompensa media={recompensa_media:.2f} +/- {recompensa_desviacion_estandar:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b274c5ba-7e6f-4e7d-841a-c379fdd1f4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fcad3add5a49f0a6c25e05325ba59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "9\n",
      "13\n",
      "14\n",
      "15\n",
      "Recompensa media=1.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "recompensa_media, recompensa_desviacion_estandar = evaluar_agente(ambiente, max_pasos, 1, frozenlake_tablaQ, semilla_evaluacion)\n",
    "print(f\"Recompensa media={recompensa_media:.2f} +/- {recompensa_desviacion_estandar:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e8c52-604c-4602-ac10-1c7872326621",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='../figuras/frozenlakeRun.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f840d96-a43c-4723-bfef-d7d90fd74d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
