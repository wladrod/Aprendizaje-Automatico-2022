{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 4: Redes Neuronales</h1>\n",
    "    <h1>Redes Neuronales Recurrentes</h1>\n",
    "    <br>\n",
    "    <h5>Prof. Wladimir Rodríguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los seres humanos no comienzan su pensamiento desde cero cada segundo. Al leer este ensayo, entiendes cada palabra según tu comprensión de las palabras anteriores. No tiras todo y comienzas a pensar de nuevo. Tus pensamientos tienen persistencia.\n",
    "\n",
    "Las redes neuronales tradicionales no pueden hacer esto, y parece una deficiencia importante. Por ejemplo, imagine que desea clasificar qué tipo de evento está sucediendo en cada punto de una película. No está claro cómo una red neuronal tradicional podría usar su razonamiento sobre eventos previos en la película para informar a los posteriores.\n",
    "\n",
    "Las redes neuronales recurrentes abordan este problema. Son redes con bucles que permiten que la información persista.\n",
    "\n",
    "<img src=\"../figuras/nodo_rnr.png\" width=\"15%\">\n",
    "\n",
    "En la figura de arriba, un trozo de la red neuronal RN, observa una entrada $x_t$ y genera un valor $s_t$. Un ciclo permite que la información pase de un paso de la red al siguiente.\n",
    "\n",
    "Estos bucles hacen que las redes neuronales recurrentes parezcan misteriosas. Sin embargo, si piensa un poco más, resulta que no son tan diferentes a una red neuronal normal. Una red neuronal recurrente se puede considerar como copias múltiples de la misma red, cada una pasando un mensaje a un sucesor. Considere lo que sucede si desenrollamos el ciclo:\n",
    "\n",
    "<img src=\"../figuras/rnr_desenrrollada.png\" width=\"75%\">\n",
    "\n",
    "Esta naturaleza de cadena revela que las redes neuronales recurrentes están íntimamente relacionadas con secuencias y listas. Son la arquitectura natural de una red neuronal para usar para tales datos.\n",
    "\n",
    "En los últimos años, ha habido un éxito increíble aplicando RNR a una variedad de problemas: reconocimiento de voz, modelado de lenguaje, traducción, subtitulado de imágenes ...\n",
    "\n",
    "Esencial para estos éxitos es el uso de LSTM (Long Short-Time Memory), un tipo muy especial de red neuronal recurrente que funciona, para muchas tareas, mucho mejor que la versión estándar. Casi todos los mejores resultados basados en redes neuronales recurrentes se logran con ellas. Son estas LSTM las que se explorará a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El Problema de las Dependencias a Largo Plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los atractivos de las RNR es la idea de que podrían ser capaces de conectar información previa a la tarea actual, por ejemplo, el uso de los cuadros de video previos podría ayudar a comprender el cuadro actual. Si las RNR pudieran hacer esto, serían extremadamente útiles.\n",
    "\n",
    "Algunas veces, solo necesitamos ver información reciente para realizar la tarea actual. Por ejemplo, considere un modelo de lenguaje que intente predecir la siguiente palabra en función de las anteriores. Si estamos tratando de predecir la última palabra en \"las nubes están en el *cielo*\", no necesitamos ningún contexto adicional; es bastante obvio que la próxima palabra será cielo. En tales casos, cuando la brecha entre la información relevante y el lugar que se necesita es pequeña, las RNR pueden aprender a usar la información pasada.\n",
    "\n",
    "<img src=\"../figuras/dependencia_corta.png\" width=\"75%\">\n",
    "\n",
    "Pero también hay casos en los que necesitamos más contexto. Considere tratar de predecir la última palabra en el texto \"Crecí en Francia ... Hablo *francés* con fluidez\". La información reciente sugiere que la siguiente palabra es probablemente el nombre de un idioma, pero si queremos reducir el idioma, necesitamos el contexto de Francia, desde atrás. Es completamente posible que exista una brecha entre la información relevante y el punto en el que se necesita que sea muy grande.\n",
    "\n",
    "Desafortunadamente, a medida que crece esa brecha, las RNN no pueden aprender a conectar la información.\n",
    "\n",
    "<img src=\"../figuras/dependencia_larga.png\" width=\"75%\">\n",
    "\n",
    "En teoría, las RNR son absolutamente capaces de manejar tales \"dependencias a largo plazo\". Un humano podría elegir cuidadosamente los parámetros para que resuelvan problemas de juguete de esta forma. Tristemente, en la práctica, las RNR no parecen ser capaces de aprenderlas. El problema fue explorado en profundidad por Hochreiter (1991) [alemán] y Bengio, et al. (1994), quienes encontraron algunas razones bastante fundamentales por las cuales podría ser difícil.\n",
    "\n",
    "Este problema se puede resolver con las LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes de memoria de corto y largo plazo, generalmente llamadas \"LSTM\" del ingles (Long Short Term Memory), son un tipo especial de RNR, capaz de aprender dependencias a largo plazo. Fueron introducidos por Hochreiter y Schmidhuber (1997), y fueron refinadas y popularizadas por muchas personas en trabajos posteriores. Funcionan muy bien en una gran variedad de problemas, y ahora son ampliamente utilizadas.\n",
    "\n",
    "Las LSTM están diseñados explícitamente para evitar el problema de dependencia a largo plazo. Recordar la información durante largos períodos de tiempo es prácticamente su comportamiento predeterminado.\n",
    "\n",
    "Todas las redes neuronales recurrentes tienen la forma de una cadena de módulos repetitivos de red neuronal. En RNN estándar, este módulo de repetición tendrá una estructura muy simple, como una sola capa de tanh.\n",
    "\n",
    "<img src=\"../figuras/estandar_rnr.png\" width=\"75%\">\n",
    "\n",
    "Los LSTM también tienen esta estructura tipo cadena, pero el módulo de repetición tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, hay cuatro, interactuando de una manera muy especial.\n",
    "\n",
    "<img src=\"../figuras/lstm_rnr.png\" width=\"100%\">\n",
    "\n",
    "Notación:\n",
    "\n",
    "<img src=\"../figuras/notacion_lstm.png\" width=\"75%\">\n",
    "\n",
    "En la figura de arriba, cada línea lleva un vector completo, desde la salida de un nodo hasta las entradas de otros. Los círculos verdes representan operaciones puntuales, como la adición de vectores, mientras que los cuadros amarillos son capas entrenadas de redes neuronales. Las líneas que se unen denotan concatenación, mientras que una línea de bifurcación denota que su contenido se copia y las copias van a diferentes ubicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La idea central detrás de los LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave para las LSTM es el estado de la celda, la línea horizontal que se extiende por la parte superior de la figura.\n",
    "\n",
    "El estado de la celda es como una cinta transportadora. Corre directamente por toda la cadena, con solo algunas interacciones lineales menores. Es muy fácil que la información fluya sin cambios\n",
    "\n",
    "<img src=\"../figuras/estado_celda.png\" width=\"50%\">\n",
    "\n",
    "La LSTM tiene la capacidad de eliminar o agregar información al estado de la celda, cuidadosamente regulado por estructuras llamadas compuertas.\n",
    "\n",
    "Las compuertas son una forma de dejar pasar la información. Se componen de una capa de red neuronal con activación sigmoide y una operación de multiplicación puntual.\n",
    "\n",
    "<img src=\"../figuras/compuerta_lstm.png\" width=\"15%\">\n",
    "\n",
    "La salida de capa con activación sigmoide son números entre cero y uno, que describe la cantidad que se debe dejar pasar cada componente. Un valor de cero significa \"no dejar pasar nada\", mientras que un valor de uno significa \"dejar pasar todo\"\n",
    "\n",
    "Una LSTM tiene tres de estas compuertas, para proteger y controlar el estado de la celda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorrido paso a paso de una LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso en nuestra LSTM es decidir qué información vamos a olvidar/recordar del estado de la celda. Esta decisión es tomada por una capa con activación sigmoide llamada \"capa de compuerta de olvido\". Basado en los valores de $s_{t-1}$ y $x_t$, genera un número entre 0 y 1 para cada número del estado de la celda $C_{t-1}$. Un 1 representa \"mantener esto por completo\", mientras que un 0 representa \"deshacerse por completo de esto\".\n",
    "\n",
    "Regresemos a nuestro ejemplo de un modelo de lenguaje que intenta predecir la siguiente palabra en base a todas las anteriores. En tal problema, el estado de la celda puede incluir el género del sujeto presente, de modo que puedan usarse los pronombres correctos. Cuando vemos un sujeto nuevo, queremos olvidar el género del sujeto anterior.\n",
    "\n",
    "<img src=\"../figuras/compuerta_olvidar.png\" width=\"75%\">\n",
    "\n",
    "El siguiente paso es decidir qué nueva información vamos a almacenar en el estado de la celda. Esto tiene dos partes. Primero, una capa sigmoide llamada \"compuerta capa de entrada\" decide qué valores actualizaremos. A continuación, una capa $tanh$ crea un vector de nuevos valores candidatos, $\\tilde{C}_t$, que podrían agregarse al estado. En el siguiente paso, combinaremos estos dos para crear la actualización del estado.\n",
    "\n",
    "En el ejemplo de nuestro modelo de lenguaje, nos gustaría agregar el género del nuevo sujeto al estado de la celda, para reemplazar el anterior que estamos olvidando.\n",
    "\n",
    "<img src=\"../figuras/compuerta_actualizar.png\" width=\"75%\">\n",
    "\n",
    "Ahora es el momento de actualizar el estado de celda anterior, $C_{t-1}$, en el nuevo estado de celda $C_t$. Los pasos anteriores ya decidieron qué hacer, solo tenemos que hacerlo.\n",
    "\n",
    "Multiplicamos el estado anterior por $f_t$, olvidando las cosas que decidimos olvidar antes. Luego le sumamos $i_t * \\tilde{C}_t$. Estos son los nuevos valores candidatos, ajustados por cuánto decidimos actualizar cada valor de estado.\n",
    "\n",
    "En el caso del modelo de lenguaje, aquí es donde dejamos caer la información sobre el género del sujeto anterior y agregamos la nueva información, como decidimos en los pasos anteriores.\n",
    "\n",
    "<img src=\"../figuras/actualizar_estado.png\" width=\"75%\">\n",
    "\n",
    "Finalmente, tenemos que decidir qué vamos a generar. Este resultado se basará en el estado de celda, pero será una versión filtrada. Primero, ejecutamos una capa con activación sigmoide que decide qué partes del estado de la celda vamos a generar. Luego, ponemos el estado de la celda a través de una función `tanh` (para ajustar los valores entre -1 y 1) y posteriormente se multiplican por la salida de la compuerta sigmoide, de modo que solo se genererarán las partes que decidamos.\n",
    "\n",
    "Para el ejemplo del modelo de lenguaje, dado que acaba de ver un tema, es posible que desee generar información relevante para un verbo, en caso de que eso sea lo que viene a continuación. Por ejemplo, podría mostrar si el sujeto es singular o plural, de modo que sabemos en qué forma debe conjugarse un verbo si eso es lo que sigue a continuación.\n",
    "\n",
    "<img src=\"../figuras/salida_celda.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de una Red Neuronal Recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo, utilizaremos el conjunto de datos de reseñas de clientes de Amazon que se puede encontrar en [Kaggle](https://www.kaggle.com/bittlingmayer/amazonreviews). El conjunto de datos contiene un total de 4 millones de reseñas y cada reseña está etiquetada como de sentimiento positivo o negativo.\n",
    "\n",
    "Nuestro objetivo al momento de esta implementación será crear un modelo LSTM que pueda clasificar y distinguir con precisión el sentimiento de una reseña. Para hacerlo, tendremos que comenzar con un preprocesamiento de datos, definir y entrenar el modelo, seguido de una evaluación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestros pasos de preprocesamiento de datos, usaremos expresiones regulares `re`, `numpy` y la librería `NLTK` (Natural Language Toolkit) para algunas funciones simples de ayuda de Procesamiento de Lenguaje Natural (PLN). Como los datos están comprimidos en el formato *bz2*, usaremos el módulo `bz2` de Python para leer los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wladimir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paquete `punkt` es una parte importante del proceso de *tokenización*. Por lo tanto, puede usar el administrador de descargas nltk para obtenerlo o usar la función de `download('punkt')` en la biblioteca `nltk` para obtenerlo a través del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_entrenamiento = bz2.BZ2File('../datos/amazon_reviews/train.ft.txt.bz2')\n",
    "archivo_prueba = bz2.BZ2File('../datos/amazon_reviews/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_entrenamiento = archivo_entrenamiento.readlines()\n",
    "archivo_prueba = archivo_prueba.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de reseñas de entrenamiento: 3600000\n",
      "Número de reseñas de prueba: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de reseñas de entrenamiento: \" + str(len(archivo_entrenamiento)))\n",
    "print(\"Número de reseñas de prueba: \" + str(len(archivo_prueba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos contiene un total de 4 millones de reseñas: 3,6 millones de entrenamiento y 0,4 millones de prueba. No usaremos todo el conjunto de datos para ahorrar tiempo. Sin embargo, si tiene el poder y la capacidad de cómputo, continúe y entrene el modelo en una porción más grande de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_entrenamiento = 800000 #Entrenaremos con las primeras 800.000 reseñas\n",
    "numero_prueba = 200000 #Usaremos las primeras 200.000 reseñas \n",
    "\n",
    "archivo_entrenamiento = [x.decode('utf-8') for x in archivo_entrenamiento[:numero_entrenamiento]]\n",
    "archivo_prueba = [x.decode('utf-8') for x in archivo_prueba[:numero_prueba]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(archivo_entrenamiento[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, tendremos que extraer las etiquetas de las oraciones. Los datos tienen el formato `__label__1/2 <oración>`, por lo que podemos dividirlos fácilmente en consecuencia. Las etiquetas de opiniones positivas se almacenan como 1 y las negativas como 0.\n",
    "\n",
    "También cambiaremos todas las URL a una \"<url>\" estándar, ya que la URL exacta es irrelevante para el sentimiento en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extrayendo etiquetas de las oraciones\n",
    "\n",
    "etiquetas_entrenamiento = [0 if x.split(' ')[0] == '__label__1' else 1 for x in archivo_entrenamiento]\n",
    "oraciones_entrenamiento = [x.split(' ', 1)[1][:-1].lower() for x in archivo_entrenamiento]\n",
    "\n",
    "    \n",
    "etiquetas_prueba = [0 if x.split(' ')[0] == '__label__1' else 1 for x in archivo_prueba]\n",
    "oraciones_prueba = [x.split(' ', 1)[1][:-1].lower() for x in archivo_prueba]\n",
    "\n",
    "# Una limpeza de datos sencilla \n",
    "\n",
    "for i in range(len(oraciones_entrenamiento)):\n",
    "    oraciones_entrenamiento[i] = re.sub('\\d','0',oraciones_entrenamiento[i])\n",
    "\n",
    "for i in range(len(oraciones_prueba)):\n",
    "    oraciones_prueba[i] = re.sub('\\d','0',oraciones_prueba[i])\n",
    "\n",
    "# Modificar las URL a <url>\n",
    "\n",
    "for i in range(len(oraciones_entrenamiento)):\n",
    "    if 'www.' in oraciones_entrenamiento[i] or 'http:' in oraciones_entrenamiento[i] or 'https:' in oraciones_entrenamiento[i] or '.com' in oraciones_entrenamiento[i]:\n",
    "        oraciones_entrenamiento[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", oraciones_entrenamiento[i])\n",
    "        \n",
    "for i in range(len(oraciones_prueba)):\n",
    "    if 'www.' in oraciones_prueba[i] or 'http:' in oraciones_prueba[i] or 'https:' in oraciones_prueba[i] or '.com' in oraciones_prueba[i]:\n",
    "        oraciones_prueba[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", oraciones_prueba[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del archivo_entrenamiento, archivo_prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de limpiar rápidamente los datos, haremos la tokenización de las oraciones, que es una tarea estándar de PNL.\n",
    "La tokenización es la tarea de dividir una oración en tokens individuales, que pueden ser palabras o puntuación, etc.\n",
    "Hay muchas bibliotecas NLP que pueden hacer esto, como *spaCy* o *Scikit-learn*, pero aquí usaremos *NLTK* ya que tiene uno de los tokenizadores más rápidos.\n",
    "\n",
    "Luego, las palabras se almacenarán en un diccionario asignando la palabra a su número de apariciones. Estas palabras se convertirán en nuestro **vocabulario**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% hecho\n",
      "2.5% hecho\n",
      "5.0% hecho\n",
      "7.5% hecho\n",
      "10.0% hecho\n",
      "12.5% hecho\n",
      "15.0% hecho\n",
      "17.5% hecho\n",
      "20.0% hecho\n",
      "22.5% hecho\n",
      "25.0% hecho\n",
      "27.5% hecho\n",
      "30.0% hecho\n",
      "32.5% hecho\n",
      "35.0% hecho\n",
      "37.5% hecho\n",
      "40.0% hecho\n",
      "42.5% hecho\n",
      "45.0% hecho\n",
      "47.5% hecho\n",
      "50.0% hecho\n",
      "52.5% hecho\n",
      "55.0% hecho\n",
      "57.5% hecho\n",
      "60.0% hecho\n",
      "62.5% hecho\n",
      "65.0% hecho\n",
      "67.5% hecho\n",
      "70.0% hecho\n",
      "72.5% hecho\n",
      "75.0% hecho\n",
      "77.5% hecho\n",
      "80.0% hecho\n",
      "82.5% hecho\n",
      "85.0% hecho\n",
      "87.5% hecho\n",
      "90.0% hecho\n",
      "92.5% hecho\n",
      "95.0% hecho\n",
      "97.5% hecho\n",
      "100% hecho\n"
     ]
    }
   ],
   "source": [
    "palabras = Counter() #Diccionario que mapea una palabra a el número de veces que aparece en todas las oraciones de entrenamiento\n",
    "for i, oracion in enumerate(oraciones_entrenamiento):\n",
    "    #Las oraciones se almacenarán en una lista de palabras/tokens\n",
    "    oraciones_entrenamiento[i] = []\n",
    "    for palabra in nltk.word_tokenize(oracion): #Tokenización de palabras\n",
    "        palabras.update([palabra.lower()]) #Convertir todas las palabras a minúsculas\n",
    "        oraciones_entrenamiento[i].append(palabra)\n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/numero_entrenamiento) + \"% hecho\")\n",
    "print(\"100% hecho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar errores tipográficos y palabras que probablemente no existan, eliminaremos todas las palabras del vocabulario que solo aparecen una vez.\n",
    "Para tener en cuenta las palabras **desconocidas** y el **relleno**, también tendremos que agregarlas a nuestro vocabulario. A cada palabra del vocabulario se le asignará un índice de número entero y, a partir de entonces, se asignará a este número entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover las palabras que solo aparecen una vez\n",
    "palabras = {k:v for k,v in palabras.items() if v>1}\n",
    "# Ordenar las palabras de acuerdo al número de apariciones\n",
    "palabras = sorted(palabras, key=palabras.get, reverse=True) #De mayor a menor\n",
    "# Agregar padding and unknown a nuestro vocabulario para asignarles un indice\n",
    "palabras = ['_PAD','_UNK'] + palabras\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "palabra2idx = {o:i for i,o in enumerate(palabras)}\n",
    "idx2palabra = {i:o for i,o in enumerate(palabras)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las asignaciones, convertiremos las palabras de las oraciones a sus índices correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, oracion in enumerate(oraciones_entrenamiento):\n",
    "    # Asignar el indice alas palabras respectivas\n",
    "    oraciones_entrenamiento[i] = [palabra2idx[palabra] if palabra in palabra2idx else palabra2idx['_UNK'] for palabra in oracion]\n",
    "\n",
    "for i, oracion in enumerate(oraciones_prueba):\n",
    "    # Para las oraciones de prueba, tenemos que tokenizar las oraciones también\n",
    "    oraciones_prueba[i] = [palabra2idx[palabra.lower()] if palabra.lower() in palabra2idx else palabra2idx['_UNK'] for palabra in nltk.word_tokenize(oracion)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el último paso de preprocesamiento, rellenaremos las oraciones con 0 y acortaremos las oraciones largas para que los datos se puedan entrenar en lotes para acelerar las cosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función que o acorte la sentencia o agregue 0 (padding) a un largo determinado\n",
    "\n",
    "def pad_entrada(oraciones, largo_secuencia):\n",
    "    atributos = np.zeros((len(oraciones), largo_secuencia),dtype=int)\n",
    "    for ii, review in enumerate(oraciones):\n",
    "        if len(review) != 0:\n",
    "            atributos[ii, -len(review):] = np.array(review)[:largo_secuencia]\n",
    "    return atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "largo_secuencia = 200\n",
    "\n",
    "oraciones_entrenamiento = pad_entrada(oraciones_entrenamiento, largo_secuencia)\n",
    "oraciones_prueba = pad_entrada(oraciones_prueba, largo_secuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir nuestras etiquetas a arreglos numpy \n",
    "etiquetas_entrenamiento = np.array(etiquetas_entrenamiento)\n",
    "etiquetas_prueba = np.array(etiquetas_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una oración con relleno se verá así, donde 0 representa el relleno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,    40,    99,    13,    28,  1447,  4272,    57,\n",
       "          31,    10,     3,    40,  1781,    10,    85,  1730,     2,\n",
       "           5,    27,   907,     8,    11,    99,    16,   151,     6,\n",
       "           5,   140,    90,     9,     2,    68,     5,   122,    14,\n",
       "           7,    42,  1847,     9,   210,    58,   243,   108,     2,\n",
       "           7,   133,  1847,    46, 29316,    38,  2642,    14,     3,\n",
       "        2379,     2,    11,    99,    46, 18846,   160,     2,   934,\n",
       "          30,     1,     1,     6,   560,    46,  1285,     2,    31,\n",
       "          10,   160,    21,  2336,  4156,     2,    11,    12,     7,\n",
       "        3570, 14981,    99,    14,    28,    24,     2,   182,   102,\n",
       "         130,   147,     9,   239,    12,    46,   827,    58,     2,\n",
       "        2587,     5,   263,    11,     4,    72,   601,   444,     4,\n",
       "         579,     4,   416,     4,   153,     4,  1689,     4,  1255,\n",
       "        1816,   521,    31,   179,    33,    80,    18,    17,   829,\n",
       "          61,    32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_prueba[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nuestro conjunto de datos ya está dividido en datos de *entrenamiento* y *pruebas*. Sin embargo, todavía necesitamos un conjunto de datos para la validación durante el entrenamiento. Por lo tanto, dividiremos nuestros datos de prueba a la mitad en un conjunto de validación y un conjunto de prueba. Se puede encontrar una explicación detallada sobre las divisiones de conjuntos de datos [aquí](https://machinelearningmastery.com/difference-test-validation-datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_division = 0.5\n",
    "id_division = int(frac_division * len(oraciones_prueba))\n",
    "oraciones_validacion, oraciones_prueba = oraciones_prueba[:id_division], oraciones_prueba[id_division:]\n",
    "etiquetas_validacion, etiquetas_prueba = etiquetas_prueba[:id_division], etiquetas_prueba[id_division:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convertir los datos a `Datasets` y `DataLoader` de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, comenzaremos a trabajar con la librería PyTorch. Primero definiremos los conjuntos de datos de las oraciones y las etiquetas, y luego los cargaremos en un cargador de datos. Establecemos el tamaño del lote en 256. Esto se puede modificar según sus necesidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "datos_entrenamiento = TensorDataset(torch.from_numpy(oraciones_entrenamiento), torch.from_numpy(etiquetas_entrenamiento))\n",
    "datos_validacion = TensorDataset(torch.from_numpy(oraciones_validacion), torch.from_numpy(etiquetas_validacion))\n",
    "datos_prueba = TensorDataset(torch.from_numpy(oraciones_prueba), torch.from_numpy(etiquetas_prueba))\n",
    "\n",
    "tamaño_lote = 400\n",
    "\n",
    "cargador_entrenamiento = DataLoader(datos_entrenamiento, shuffle=True, batch_size=tamaño_lote)\n",
    "cargador_validacion = DataLoader(datos_validacion, shuffle=True, batch_size=tamaño_lote)\n",
    "cargador_prueba = DataLoader(datos_prueba, shuffle=True, batch_size=tamaño_lote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos verificar si tenemos alguna GPU para acelerar nuestro tiempo de entrenamiento en muchos pliegues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    dispositivo = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    dispositivo = \"cuda\"\n",
    "else: \"cpu\"\n",
    "dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 200]) torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "iterador_datos = iter(cargador_entrenamiento)\n",
    "ejemplo_x, ejemplo_y = iterador_datos.next()\n",
    "\n",
    "print(ejemplo_x.shape, ejemplo_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, definiremos la arquitectura del modelo. En esta etapa, podemos crear redes neuronales que tengan capas profundas o una gran cantidad de capas LSTM apiladas una encima de la otra. Sin embargo, un modelo simple como el que se muestra a continuación funciona bastante bien y requiere mucho menos tiempo de entrenamiento. Estaremos entrenando nuestras propios *words embedings* de palabras en la primera capa antes de que las oraciones se introduzcan en la capa LSTM.\n",
    "\n",
    "La capa final es una capa totalmente conectada con una función sigmoidea para clasificar si la reseña es de opinión positiva o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RedAnalisisSentimientos(nn.Module):\n",
    "    def __init__(self, tamaño_vocabulario, tamaño_salida, dim_embedding, dim_oculta, numero_capas, drop_prob=0.5):\n",
    "        super(RedAnalisisSentimientos, self).__init__()\n",
    "        self.tamaño_salida = tamaño_salida\n",
    "        self.numero_capas = numero_capas\n",
    "        self.dim_oculta = dim_oculta\n",
    "        \n",
    "        self.embedding = nn.Embedding(tamaño_vocabulario, dim_embedding)\n",
    "        self.lstm = nn.LSTM(dim_embedding, dim_oculta, numero_capas, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(dim_oculta, tamaño_salida)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, oculta):\n",
    "        tamaño_lote = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_salida, oculta = self.lstm(embeds, oculta)\n",
    "        lstm_salida = lstm_salida.contiguous().view(-1, self.dim_oculta)\n",
    "        \n",
    "        salida = self.dropout(lstm_salida)\n",
    "        salida = self.fc(salida)\n",
    "        salida = self.sigmoid(salida)\n",
    "        \n",
    "        salida = salida.view(tamaño_lote, -1)\n",
    "        salida = salida[:,-1]\n",
    "        return salida, oculta\n",
    "    \n",
    "    def inicializar_oculta(self, tamaño_lote):\n",
    "        pesos = next(self.parameters()).data\n",
    "        oculta = (pesos.new(self.numero_capas, tamaño_lote, self.dim_oculta).zero_().to(dispositivo),\n",
    "                      pesos.new(self.numero_capas, tamaño_lote, self.dim_oculta).zero_().to(dispositivo))\n",
    "        return oculta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que en realidad podemos cargar *word embeddings* de palabras previamente entrenadas, como GloVe o fastText, que pueden aumentar la precisión del modelo y disminuir el tiempo de entrenamiento.\n",
    "\n",
    "Con esto, podemos instanciar nuestro modelo después de definir los argumentos. La dimensión de salida solo será 1, ya que solo necesita generar 1 o 0. La tasa de aprendizaje, la función de pérdida y el optimizador también serán definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedAnalisisSentimientos(\n",
      "  (embedding): Embedding(221606, 400)\n",
      "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tamaño_vocabulario = len(palabra2idx) + 1\n",
    "tamaño_salida = 1\n",
    "dim_embedding = 400\n",
    "dim_oculta = 512\n",
    "numero_capas = 2\n",
    "\n",
    "modelo = RedAnalisisSentimientos(tamaño_vocabulario, tamaño_salida, dim_embedding, dim_oculta, numero_capas)\n",
    "modelo.to(dispositivo)\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "taza_aprendizaje = 0.005\n",
    "fn_perdida = nn.BCELoss()\n",
    "optimizador = torch.optim.Adam(modelo.parameters(), lr=taza_aprendizaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos comenzar a entrenar el modelo. Por cada 1000 pasos, verificaremos el resultado de nuestro modelo con el conjunto de datos de validación y guardaremos el modelo si se desempeñó mejor que la vez anterior. El `state_dict` son los pesos del modelo en PyTorch y se puede posteriormente cargar en un modelo con la misma arquitectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 2\n",
    "contador = 0\n",
    "imprimir_cada = 1000\n",
    "clip = 5\n",
    "perdida_min_validacion = np.Inf\n",
    "\n",
    "modelo.train()\n",
    "for i in range(epocas):\n",
    "    h = modelo.inicializar_oculta(tamaño_lote)\n",
    "    \n",
    "    for entradas, etiquetas in cargador_entrenamiento:\n",
    "        contador += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        entradas, etiquetas = entradas.to(dispositivo), etiquetas.to(dispositivo)\n",
    "        modelo.zero_grad()\n",
    "        salida, h = modelo(entradas, h)\n",
    "        perdida = fn_perdida(salida.squeeze(), etiquetas.float())\n",
    "        perdida.backward()\n",
    "        nn.utils.clip_grad_norm_(modelo.parameters(), clip)\n",
    "        optimizador.step()\n",
    "        \n",
    "        if contador%imprimir_cada == 0:\n",
    "            h_val = modelo.inicializar_oculta(tamaño_lote)\n",
    "            perdidas_val = []\n",
    "            modelo.eval()\n",
    "            for entrada, etiqueta in cargador_validacion:\n",
    "                h_val = tuple([cada.data for cada in h_val])\n",
    "                entrada, etiqueta = entrada.to(dispositivo), etiqueta.to(dispositivo)\n",
    "                salida, h_val = modelo(entrada, h_val)\n",
    "                perdida_val = fn_perdida(salida.squeeze(), etiqueta.float())\n",
    "                perdidas_val.append(perdida_val.item())\n",
    "                \n",
    "            modelo.train()\n",
    "            print(f'Epoca: {i+1}/{epocas}...',\n",
    "                  f'Paso: {contador}...',\n",
    "                  f'Perdida: {perdida.item():.6f}...',\n",
    "                  f'Perdida Val: {np.mean(perdidas_val):.6f}')\n",
    "            if np.mean(perdidas_val) <= perdida_min_validacion:\n",
    "                torch.save(modelo.state_dict(), '../modelos/RedAnalisisSentimientos.pt')\n",
    "                print(f'Validation loss decreased ({perdida_min_validacion:.6f} --> {np.mean(perdidas_val):.6f}.  Guardando modelo ...')\n",
    "                perdida_min_validacion = np.mean(perdidas_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayamos terminado el entrenamiento, es hora de probar nuestro modelo en un conjunto de datos que nunca antes había visto: nuestro conjunto de datos de prueba.\n",
    "Primero cargaremos los pesos del modelo desde el punto donde la pérdida de validación es la más baja.\n",
    "Podemos calcular la precisión del modelo para ver qué tan precisas son las predicciones de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cargar el mejor modelo\n",
    "modelo.load_state_dict(torch.load('../modelos/RedAnalisisSentimientos.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdida prueba: 0.737\n",
      "Exactitud prueba: 47.654%\n"
     ]
    }
   ],
   "source": [
    "perdidas_prueba = []\n",
    "numero_correctos = 0\n",
    "h = modelo.inicializar_oculta(tamaño_lote)\n",
    "\n",
    "modelo.eval()\n",
    "for entradas, etiquetas in cargador_prueba:\n",
    "    h = tuple([cada.data for cada in h])\n",
    "    entradas, etiquetas = entradas.to(dispositivo), etiquetas.to(dispositivo)\n",
    "    salida, h = modelo(entradas, h)\n",
    "    perdida_prueba = fn_perdida(salida.squeeze(), etiquetas.float())\n",
    "    perdidas_prueba.append(perdida_prueba.item())\n",
    "    prediccion = torch.round(salida.squeeze()) #rounds the output to 0/1\n",
    "    tensor_correcto = prediccion.eq(etiquetas.float().view_as(prediccion))\n",
    "    correcto = np.squeeze(tensor_correcto.cpu().numpy())\n",
    "    numero_correctos += np.sum(correcto)\n",
    "        \n",
    "print(f'Perdida prueba: {np.mean(perdidas_prueba):.3f}')\n",
    "exactitud_prueba = numero_correctos/len(cargador_prueba.dataset)\n",
    "print(f'Exactitud prueba: {exactitud_prueba*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
